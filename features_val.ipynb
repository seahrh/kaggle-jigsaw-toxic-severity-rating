{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b888a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.stats import rankdata\n",
    "import textstat\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, NamedTuple, Callable\n",
    "import scml\n",
    "import mylib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc324d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conf(device=device(type='cuda'), pretrained_dir='pretrained/', dtfy_model_max_length=512, dtfy_batch_size=64, dtfy_models={'dto_': 'pretrained/unitaryai/detoxify/toxic_original-c1212f89.ckpt', 'dtu_': 'pretrained/unitaryai/detoxify/toxic_debiased-c7548aa0.ckpt', 'dtm_': 'pretrained/unitaryai/detoxify/multilingual_debiased-0b549669.ckpt'}, dtfy_configs={'dto_': 'pretrained/bert-base-uncased', 'dtu_': 'pretrained/roberta-base', 'dtm_': 'pretrained/xlm-roberta-base'}, tweeteval_model_max_length=512, tweeteval_batch_size=64, tweeteval_models={'te_roberta_off': 'pretrained/cardiffnlp/twitter-roberta-base-offensive', 'te_roberta_emo_anger': 'pretrained/cardiffnlp/twitter-roberta-base-emotion', 'te_roberta_snt_neg': 'pretrained/cardiffnlp/twitter-roberta-base-sentiment', 'te_roberta_iro': 'pretrained/cardiffnlp/twitter-roberta-base-irony', 'te_xlm_roberta_snt_neg': 'pretrained/cardiffnlp/twitter-xlm-roberta-base-sentiment'}, tweeteval_label_index={'te_roberta_off': 1, 'te_roberta_emo_anger': 0, 'te_roberta_snt_neg': 0, 'te_roberta_iro': 1, 'te_xlm_roberta_snt_neg': 0}, hatebert_model_max_length=512, hatebert_batch_size=128, hatebert_models={'hb_bert_off': 'pretrained//hatebert/bert-offenseval', 'hb_bert_abu': 'pretrained//hatebert/bert-abuseval', 'hb_hatebert_off': 'pretrained//hatebert/hatebert-offenseval', 'hb_hatebert_abu': 'pretrained//hatebert/hatebert-abuseval'}, em_max_seq_length=128, em_batch_size=1000, em_models={'paraphrase-MiniLM-L6-v2': 'pretrained/sentence-transformers/paraphrase-MiniLM-L6-v2'})\n",
      "device=0, NVIDIA GeForce GTX 1060 6GB\n",
      "Mem Allocated: 0.0 GB\n",
      "Mem Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "class Conf(NamedTuple):\n",
    "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    pretrained_dir: str = \"pretrained/\"\n",
    "    dtfy_model_max_length: int = 512\n",
    "    dtfy_batch_size: int = 64\n",
    "    dtfy_models: Dict[str, str] = {\n",
    "        \"dto_\": f\"{pretrained_dir}unitaryai/detoxify/toxic_original-c1212f89.ckpt\",\n",
    "        \"dtu_\": f\"{pretrained_dir}unitaryai/detoxify/toxic_debiased-c7548aa0.ckpt\",\n",
    "        \"dtm_\": f\"{pretrained_dir}unitaryai/detoxify/multilingual_debiased-0b549669.ckpt\"\n",
    "    }\n",
    "    dtfy_configs: Dict[str, str] = {\n",
    "        \"dto_\": f\"{pretrained_dir}bert-base-uncased\",\n",
    "        \"dtu_\": f\"{pretrained_dir}roberta-base\",\n",
    "        \"dtm_\": f\"{pretrained_dir}xlm-roberta-base\"\n",
    "    }\n",
    "    tweeteval_model_max_length: int = 512\n",
    "    tweeteval_batch_size: int = 64\n",
    "    tweeteval_models: Dict[str, str] = {\n",
    "        \"te_roberta_off\": f\"{pretrained_dir}cardiffnlp/twitter-roberta-base-offensive\",\n",
    "        \"te_roberta_emo_anger\": f\"{pretrained_dir}cardiffnlp/twitter-roberta-base-emotion\",\n",
    "        \"te_roberta_snt_neg\": f\"{pretrained_dir}cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "        \"te_roberta_iro\": f\"{pretrained_dir}cardiffnlp/twitter-roberta-base-irony\",\n",
    "        \"te_xlm_roberta_snt_neg\": f\"{pretrained_dir}cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
    "    }\n",
    "    tweeteval_label_index: Dict[str, int] = {\n",
    "        \"te_roberta_off\": 1,\n",
    "        \"te_roberta_emo_anger\": 0,\n",
    "        \"te_roberta_snt_neg\": 0,\n",
    "        \"te_roberta_iro\": 1,\n",
    "        \"te_xlm_roberta_snt_neg\": 0,\n",
    "    }\n",
    "    hatebert_model_max_length: int = 512\n",
    "    hatebert_batch_size: int = 128\n",
    "    hatebert_models: Dict[str, str] = {\n",
    "        \"hb_bert_off\": f\"{pretrained_dir}/hatebert/bert-offenseval\",\n",
    "        \"hb_bert_abu\" : f\"{pretrained_dir}/hatebert/bert-abuseval\",\n",
    "        \"hb_hatebert_off\": f\"{pretrained_dir}/hatebert/hatebert-offenseval\",\n",
    "        \"hb_hatebert_abu\" : f\"{pretrained_dir}/hatebert/hatebert-abuseval\",\n",
    "    }\n",
    "    em_max_seq_length: int = 128\n",
    "    em_batch_size: int = 1000\n",
    "    em_models: Dict[str, str] = {\n",
    "        \"paraphrase-MiniLM-L6-v2\": f\"{pretrained_dir}sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "    }\n",
    "        \n",
    "        \n",
    "conf = Conf()\n",
    "print(conf)\n",
    "if conf.device.type == 'cuda':\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"device={i}, {torch.cuda.get_device_name(i)}\")\n",
    "        print('Mem Allocated:', round(torch.cuda.memory_allocated(i)/1024**3,1), 'GB')\n",
    "        print('Mem Cached:   ', round(torch.cuda.memory_reserved(i)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "769822a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles=[.01, .05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99]\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "pd.set_option(\"use_inf_as_na\", True)\n",
    "pd.set_option(\"max_info_columns\", 9999)\n",
    "pd.set_option(\"display.max_columns\", 9999)\n",
    "pd.set_option(\"display.max_rows\", 9999)\n",
    "pd.set_option('max_colwidth', 9999)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce0265c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14251 entries, 0 to 14250\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    14251 non-null  object\n",
      " 1   text1   14251 non-null  object\n",
      " 2   text2   14251 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 334.1+ KB\n",
      "Wall time: 261 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\"input/pre_val.parquet\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae8823",
   "metadata": {},
   "source": [
    "# Character level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3f86636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "col = \"length\"\n",
    "df[col] = df[\"text1\"].str.len()\n",
    "df[col] = df[col].astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2106ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_frac(row) -> float:\n",
    "    return mylib.digit_frac(row[\"text1\"])\n",
    "\n",
    "\n",
    "def letter_frac(row) -> float:\n",
    "    return mylib.letter_frac(row[\"text1\"])\n",
    "\n",
    "\n",
    "def space_frac(row) -> float:\n",
    "    return mylib.space_frac(row[\"text1\"])\n",
    "\n",
    "\n",
    "def punc_frac(row) -> float:\n",
    "    return mylib.punc_frac(row[\"text1\"])\n",
    "\n",
    "\n",
    "def upper_frac(row) -> float:\n",
    "    return mylib.upper_frac(row[\"text1\"])\n",
    "\n",
    "\n",
    "char_fns: Dict[str, Callable] = {\n",
    "    \"digit_frac\": digit_frac,\n",
    "    \"letter_frac\": letter_frac,\n",
    "    \"space_frac\": space_frac,\n",
    "    \"punc_frac\": punc_frac,\n",
    "    \"upper_frac\": upper_frac,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24268d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digit_frac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 14251/14251 [00:00<00:00, 23962.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter_frac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 14251/14251 [00:00<00:00, 23169.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space_frac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 14251/14251 [00:00<00:00, 22767.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punc_frac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 14251/14251 [00:00<00:00, 20962.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper_frac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 14251/14251 [00:00<00:00, 23568.30it/s]\n"
     ]
    }
   ],
   "source": [
    "for col, fn in char_fns.items():\n",
    "    print(col)\n",
    "    df[col] = df.progress_apply(fn, axis=1)\n",
    "    df[col] = df[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a56aff6",
   "metadata": {},
   "source": [
    "# Textstat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03fc1f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(row) -> int:\n",
    "    return textstat.syllable_count(row[\"text1\"])\n",
    "\n",
    "\n",
    "def lexicon_count(row) -> int:\n",
    "    return textstat.lexicon_count(row[\"text1\"])\n",
    "\n",
    "\n",
    "def sentence_count(row) -> int:\n",
    "    return textstat.sentence_count(row[\"text1\"])\n",
    "\n",
    "\n",
    "def syllables_per_word(row) -> float:\n",
    "    return row[\"syllable_count\"] / (row[\"lexicon_count\"] + 1)\n",
    "\n",
    "\n",
    "def syllables_per_sent(row) -> float:\n",
    "    return row[\"syllable_count\"] / (row[\"sentence_count\"] + 1)\n",
    "\n",
    "\n",
    "def words_per_sent(row) -> float:\n",
    "    return row[\"lexicon_count\"] / (row[\"sentence_count\"] + 1)\n",
    "\n",
    "\n",
    "def flesch_reading_ease(row) -> float:\n",
    "    return textstat.flesch_reading_ease(row[\"text1\"])\n",
    "\n",
    "\n",
    "def flesch_kincaid_grade(row) -> float:\n",
    "    return textstat.flesch_kincaid_grade(row[\"text1\"])\n",
    "\n",
    "\n",
    "def gunning_fog(row) -> float:\n",
    "    return textstat.gunning_fog(row[\"text1\"])\n",
    "\n",
    "\n",
    "def smog_index(row) -> float:\n",
    "    return textstat.smog_index(row[\"text1\"])\n",
    "\n",
    "\n",
    "def automated_readability_index(row) -> float:\n",
    "    return textstat.automated_readability_index(row[\"text1\"])\n",
    "\n",
    "\n",
    "def coleman_liau_index(row) -> float:\n",
    "    return textstat.coleman_liau_index(row[\"text1\"])\n",
    "\n",
    "\n",
    "def linsear_write_formula(row) -> float:\n",
    "    return textstat.linsear_write_formula(row[\"text1\"])\n",
    "\n",
    "\n",
    "def dale_chall_readability_score(row) -> float:\n",
    "    return textstat.dale_chall_readability_score(row[\"text1\"])\n",
    "\n",
    "\n",
    "textstat_fns: Dict[str, Callable] = {\n",
    "    \"syllables_per_word\": syllables_per_word,\n",
    "    \"syllables_per_sent\": syllables_per_sent,\n",
    "    \"words_per_sent\": words_per_sent,\n",
    "    \"flesch_reading_ease\": flesch_reading_ease,\n",
    "    \"flesch_kincaid_grade\": flesch_kincaid_grade,\n",
    "    \"gunning_fog\": gunning_fog,\n",
    "    \"smog_index\": smog_index,\n",
    "    \"automated_readability_index\": automated_readability_index,\n",
    "    \"coleman_liau_index\": coleman_liau_index,\n",
    "    \"linsear_write_formula\": linsear_write_formula,\n",
    "    \"dale_chall_readability_score\": dale_chall_readability_score,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb82c1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 14251/14251 [00:02<00:00, 4960.45it/s]\n"
     ]
    }
   ],
   "source": [
    "col = \"syllable_count\"\n",
    "df[col] = df.progress_apply(syllable_count, axis=1)\n",
    "df[col] = df[col].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dc20983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 14251/14251 [00:00<00:00, 50776.15it/s]\n"
     ]
    }
   ],
   "source": [
    "col = \"lexicon_count\"\n",
    "df[col] = df.progress_apply(lexicon_count, axis=1)\n",
    "df[col] = df[col].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dff7a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 14251/14251 [00:00<00:00, 27656.00it/s]\n"
     ]
    }
   ],
   "source": [
    "col = \"sentence_count\"\n",
    "df[col] = df.progress_apply(sentence_count, axis=1)\n",
    "df[col] = df[col].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a27fe727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syllables_per_word\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 14251/14251 [00:00<00:00, 90493.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syllables_per_sent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 14251/14251 [00:00<00:00, 90354.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_per_sent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 14251/14251 [00:00<00:00, 94967.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flesch_reading_ease\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 14251/14251 [00:01<00:00, 7135.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flesch_kincaid_grade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 14251/14251 [00:01<00:00, 7309.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gunning_fog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 14251/14251 [00:02<00:00, 5673.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smog_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 14251/14251 [00:01<00:00, 8058.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automated_readability_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 14251/14251 [00:00<00:00, 19112.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coleman_liau_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 14251/14251 [00:00<00:00, 16655.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linsear_write_formula\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 14251/14251 [00:01<00:00, 9391.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dale_chall_readability_score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 14251/14251 [00:02<00:00, 6183.11it/s]\n"
     ]
    }
   ],
   "source": [
    "for col, fn in textstat_fns.items():\n",
    "    print(col)\n",
    "    df[col] = df.progress_apply(fn, axis=1)\n",
    "    df[col] = df[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49885c",
   "metadata": {},
   "source": [
    "# TweetEval labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79bd4f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 223/223 [10:33<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te_roberta_off torch.Size([14251, 2])\n",
      "logits[:10]=tensor([[0.7808, 0.2192],\n",
      "        [0.7374, 0.2626],\n",
      "        [0.5861, 0.4139],\n",
      "        [0.6431, 0.3569],\n",
      "        [0.7571, 0.2429],\n",
      "        [0.8747, 0.1253],\n",
      "        [0.6308, 0.3692],\n",
      "        [0.6307, 0.3693],\n",
      "        [0.5984, 0.4016],\n",
      "        [0.1603, 0.8397]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 223/223 [10:15<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te_roberta_emo_anger torch.Size([14251, 4])\n",
      "logits[:10]=tensor([[0.6117, 0.0391, 0.0387, 0.3105],\n",
      "        [0.3004, 0.2791, 0.1852, 0.2353],\n",
      "        [0.9812, 0.0046, 0.0058, 0.0085],\n",
      "        [0.9441, 0.0094, 0.0242, 0.0223],\n",
      "        [0.6882, 0.0294, 0.1360, 0.1464],\n",
      "        [0.9502, 0.0051, 0.0248, 0.0199],\n",
      "        [0.9512, 0.0061, 0.0251, 0.0176],\n",
      "        [0.9755, 0.0071, 0.0081, 0.0093],\n",
      "        [0.9808, 0.0038, 0.0085, 0.0069],\n",
      "        [0.9796, 0.0061, 0.0087, 0.0056]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 223/223 [10:14<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te_roberta_snt_neg torch.Size([14251, 3])\n",
      "logits[:10]=tensor([[0.3801, 0.5967, 0.0232],\n",
      "        [0.4556, 0.4819, 0.0625],\n",
      "        [0.6621, 0.2949, 0.0430],\n",
      "        [0.8434, 0.1474, 0.0092],\n",
      "        [0.6515, 0.3339, 0.0145],\n",
      "        [0.7450, 0.2245, 0.0305],\n",
      "        [0.9520, 0.0458, 0.0022],\n",
      "        [0.9025, 0.0852, 0.0124],\n",
      "        [0.8959, 0.0970, 0.0071],\n",
      "        [0.9670, 0.0289, 0.0041]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 223/223 [10:17<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te_roberta_iro torch.Size([14251, 2])\n",
      "logits[:10]=tensor([[0.7995, 0.2005],\n",
      "        [0.6654, 0.3346],\n",
      "        [0.8891, 0.1109],\n",
      "        [0.9110, 0.0890],\n",
      "        [0.9260, 0.0740],\n",
      "        [0.9104, 0.0896],\n",
      "        [0.9090, 0.0910],\n",
      "        [0.7519, 0.2481],\n",
      "        [0.2460, 0.7540],\n",
      "        [0.8470, 0.1530]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 223/223 [10:13<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te_xlm_roberta_snt_neg torch.Size([14251, 3])\n",
      "logits[:10]=tensor([[0.5742, 0.4067, 0.0191],\n",
      "        [0.5552, 0.3596, 0.0851],\n",
      "        [0.7190, 0.2055, 0.0755],\n",
      "        [0.8879, 0.0896, 0.0226],\n",
      "        [0.4847, 0.3261, 0.1892],\n",
      "        [0.8670, 0.1030, 0.0300],\n",
      "        [0.9304, 0.0527, 0.0169],\n",
      "        [0.6692, 0.2301, 0.1007],\n",
      "        [0.9190, 0.0647, 0.0163],\n",
      "        [0.9436, 0.0413, 0.0151]])\n"
     ]
    }
   ],
   "source": [
    "sentences = list(df[\"text2\"])\n",
    "for col, model_dir in conf.tweeteval_models.items():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_dir, \n",
    "        model_max_length=conf.tweeteval_model_max_length\n",
    "    )\n",
    "    #print(f\"{repr(tokenizer)}\\nmodel_input_names={tokenizer.model_input_names}\")\n",
    "    x = tokenizer(sentences, truncation=True, padding=\"max_length\")\n",
    "    batches = torch.utils.data.DataLoader(mylib.Dataset(x), batch_size=conf.tweeteval_batch_size, shuffle=False)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "    model.eval()\n",
    "    model.to(conf.device)\n",
    "    logits = None\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(batches):\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(conf.device)\n",
    "            outputs = model(**batch)\n",
    "            tmp = outputs.logits.detach().cpu()\n",
    "            if logits is None:\n",
    "                logits = tmp\n",
    "            else:\n",
    "                logits = torch.cat((logits, tmp), 0)\n",
    "    logits = torch.nn.functional.softmax(logits, dim=1)\n",
    "    print(f\"{col} {logits.size()}\\nlogits[:10]={logits[:10]}\")\n",
    "    df[col] = logits[:,conf.tweeteval_label_index[col]]\n",
    "    df[col] = df[col].astype(np.float32)\n",
    "    del tokenizer, model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73fa5a",
   "metadata": {},
   "source": [
    "# HateBert labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "993940c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='pretrained//hatebert/hatebert-offenseval', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "model_input_names=['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# all Hatebert models use the same tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    conf.hatebert_models[\"hb_hatebert_off\"], \n",
    "    model_max_length=conf.hatebert_model_max_length\n",
    ")\n",
    "print(f\"{repr(tokenizer)}\\nmodel_input_names={tokenizer.model_input_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61c3c544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "len=14251\n",
      "Wall time: 4.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = tokenizer(sentences, truncation=True, padding=\"max_length\")\n",
    "print(f\"{repr(x.keys())}\\nlen={len(x['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d459f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 112/112 [10:21<00:00,  5.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hb_bert_off torch.Size([14251, 2])\n",
      "logits[:10]=tensor([[0.9580, 0.0420],\n",
      "        [0.5313, 0.4687],\n",
      "        [0.3568, 0.6432],\n",
      "        [0.4591, 0.5409],\n",
      "        [0.2975, 0.7025],\n",
      "        [0.9420, 0.0580],\n",
      "        [0.1742, 0.8258],\n",
      "        [0.1991, 0.8009],\n",
      "        [0.2253, 0.7747],\n",
      "        [0.0307, 0.9693]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 112/112 [10:21<00:00,  5.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hb_bert_abu torch.Size([14251, 2])\n",
      "logits[:10]=tensor([[0.9930, 0.0070],\n",
      "        [0.9452, 0.0548],\n",
      "        [0.7233, 0.2767],\n",
      "        [0.8935, 0.1065],\n",
      "        [0.3779, 0.6221],\n",
      "        [0.9128, 0.0872],\n",
      "        [0.0970, 0.9030],\n",
      "        [0.1485, 0.8515],\n",
      "        [0.2820, 0.7180],\n",
      "        [0.0292, 0.9708]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 112/112 [10:20<00:00,  5.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hb_hatebert_off torch.Size([14251, 2])\n",
      "logits[:10]=tensor([[0.9157, 0.0843],\n",
      "        [0.8218, 0.1782],\n",
      "        [0.3219, 0.6781],\n",
      "        [0.3107, 0.6893],\n",
      "        [0.1201, 0.8799],\n",
      "        [0.7714, 0.2286],\n",
      "        [0.3239, 0.6761],\n",
      "        [0.1335, 0.8665],\n",
      "        [0.2956, 0.7044],\n",
      "        [0.0418, 0.9582]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 112/112 [10:20<00:00,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hb_hatebert_abu torch.Size([14251, 2])\n",
      "logits[:10]=tensor([[0.9841, 0.0159],\n",
      "        [0.9616, 0.0384],\n",
      "        [0.3335, 0.6665],\n",
      "        [0.7939, 0.2061],\n",
      "        [0.7242, 0.2758],\n",
      "        [0.9525, 0.0475],\n",
      "        [0.1641, 0.8359],\n",
      "        [0.1547, 0.8453],\n",
      "        [0.3091, 0.6909],\n",
      "        [0.0326, 0.9674]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batches = torch.utils.data.DataLoader(mylib.Dataset(x), batch_size=conf.hatebert_batch_size, shuffle=False)\n",
    "for col, model_dir in conf.hatebert_models.items():    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "    model.eval()\n",
    "    model.to(conf.device)\n",
    "    logits = None\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(batches):\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(conf.device)\n",
    "            outputs = model(**batch)\n",
    "            tmp = outputs.logits.detach().cpu()\n",
    "            if logits is None:\n",
    "                logits = tmp\n",
    "            else:\n",
    "                logits = torch.cat((logits, tmp), 0)\n",
    "    logits = torch.nn.functional.softmax(logits, dim=1)\n",
    "    print(f\"{col} {logits.size()}\\nlogits[:10]={logits[:10]}\")\n",
    "    df[col] = logits[:,1]\n",
    "    df[col] = df[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fc1d9b",
   "metadata": {},
   "source": [
    "# Detoxify labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18c17b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 3/3 [30:28<00:00, 609.53s/it]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "dtfy_fs = []\n",
    "for prefix, checkpoint in tqdm(conf.dtfy_models.items()):\n",
    "    res = mylib.detoxify_labels(\n",
    "        sentences,\n",
    "        checkpoint=checkpoint,\n",
    "        config_dir=conf.dtfy_configs[prefix],\n",
    "        model_max_length=conf.dtfy_model_max_length,\n",
    "        device=conf.device,\n",
    "        batch_size=conf.dtfy_batch_size\n",
    "    )\n",
    "    for k, v in res.items():\n",
    "        col = prefix + k\n",
    "        df[col] = v\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "        dtfy_fs.append(col)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "491bb9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dto_toxicity', 'dto_severe_toxicity', 'dto_obscene', 'dto_threat', 'dto_insult', 'dto_identity_attack', 'dtu_toxicity', 'dtu_severe_toxicity', 'dtu_obscene', 'dtu_identity_attack', 'dtu_insult', 'dtu_threat', 'dtu_sexual_explicit', 'dtm_toxicity', 'dtm_severe_toxicity', 'dtm_obscene', 'dtm_identity_attack', 'dtm_insult', 'dtm_threat', 'dtm_sexual_explicit']\n"
     ]
    }
   ],
   "source": [
    "print(dtfy_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e5b5d",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11d1f90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|SentenceTransformer.py:60] 2022-01-23 08:16:27,802 >> Load pretrained SentenceTransformer: pretrained/sentence-transformers/paraphrase-MiniLM-L6-v2\n",
      "[INFO|SentenceTransformer.py:60] 2022-01-23 08:16:27,802 >> Load pretrained SentenceTransformer: pretrained/sentence-transformers/paraphrase-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185cac9d770f450789859d25c6af8d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "em.shape=(14251, 384)\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(conf.em_models[\"paraphrase-MiniLM-L6-v2\"], device=conf.device)\n",
    "model.max_seq_length = conf.em_max_seq_length\n",
    "em = model.encode(sentences=sentences, batch_size=conf.em_batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "print(f\"em.shape={em.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73bdd7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\dev\\seahrh\\kaggle-jigsaw-toxic-severity-rating\\env\\lib\\site-packages\\pandas\\core\\frame.py:3673: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self[col] = igetitem(value, i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 291 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "em_size = em.shape[1]\n",
    "em_cols = [f\"zz{i:04d}\" for i in range(em_size)]\n",
    "df[em_cols] = em\n",
    "df[em_cols] = df[em_cols].astype(np.float32)\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ea02c",
   "metadata": {},
   "source": [
    "# Review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfdb36a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>digit_frac</th>\n",
       "      <th>letter_frac</th>\n",
       "      <th>space_frac</th>\n",
       "      <th>punc_frac</th>\n",
       "      <th>upper_frac</th>\n",
       "      <th>syllables_per_word</th>\n",
       "      <th>syllables_per_sent</th>\n",
       "      <th>words_per_sent</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>dto_toxicity</th>\n",
       "      <th>dto_severe_toxicity</th>\n",
       "      <th>dto_obscene</th>\n",
       "      <th>dto_threat</th>\n",
       "      <th>dto_insult</th>\n",
       "      <th>dto_identity_attack</th>\n",
       "      <th>dtu_toxicity</th>\n",
       "      <th>dtu_severe_toxicity</th>\n",
       "      <th>dtu_obscene</th>\n",
       "      <th>dtu_identity_attack</th>\n",
       "      <th>dtu_insult</th>\n",
       "      <th>dtu_threat</th>\n",
       "      <th>dtu_sexual_explicit</th>\n",
       "      <th>dtm_toxicity</th>\n",
       "      <th>dtm_severe_toxicity</th>\n",
       "      <th>dtm_obscene</th>\n",
       "      <th>dtm_identity_attack</th>\n",
       "      <th>dtm_insult</th>\n",
       "      <th>dtm_threat</th>\n",
       "      <th>dtm_sexual_explicit</th>\n",
       "      <th>hb_bert_off</th>\n",
       "      <th>hb_bert_abu</th>\n",
       "      <th>hb_hatebert_off</th>\n",
       "      <th>hb_hatebert_abu</th>\n",
       "      <th>te_roberta_off</th>\n",
       "      <th>te_roberta_emo_anger</th>\n",
       "      <th>te_roberta_snt_neg</th>\n",
       "      <th>te_roberta_iro</th>\n",
       "      <th>te_xlm_roberta_snt_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "      <td>14251.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>407.412111</td>\n",
       "      <td>0.009792</td>\n",
       "      <td>0.771801</td>\n",
       "      <td>0.171560</td>\n",
       "      <td>0.046848</td>\n",
       "      <td>0.087480</td>\n",
       "      <td>1.366432</td>\n",
       "      <td>18.896488</td>\n",
       "      <td>13.300020</td>\n",
       "      <td>63.766296</td>\n",
       "      <td>9.520686</td>\n",
       "      <td>11.068330</td>\n",
       "      <td>4.353828</td>\n",
       "      <td>13.135674</td>\n",
       "      <td>9.296756</td>\n",
       "      <td>9.612117</td>\n",
       "      <td>9.582970</td>\n",
       "      <td>0.453714</td>\n",
       "      <td>0.065993</td>\n",
       "      <td>0.293632</td>\n",
       "      <td>0.028148</td>\n",
       "      <td>0.271550</td>\n",
       "      <td>0.062582</td>\n",
       "      <td>0.528137</td>\n",
       "      <td>0.045724</td>\n",
       "      <td>0.293552</td>\n",
       "      <td>0.064130</td>\n",
       "      <td>0.303063</td>\n",
       "      <td>0.017880</td>\n",
       "      <td>0.108263</td>\n",
       "      <td>0.503549</td>\n",
       "      <td>0.060730</td>\n",
       "      <td>0.259472</td>\n",
       "      <td>0.057912</td>\n",
       "      <td>0.266497</td>\n",
       "      <td>0.026968</td>\n",
       "      <td>0.141305</td>\n",
       "      <td>0.637523</td>\n",
       "      <td>0.493512</td>\n",
       "      <td>0.598392</td>\n",
       "      <td>0.453566</td>\n",
       "      <td>0.538566</td>\n",
       "      <td>0.810702</td>\n",
       "      <td>0.713403</td>\n",
       "      <td>0.245387</td>\n",
       "      <td>0.740695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>687.677043</td>\n",
       "      <td>0.032306</td>\n",
       "      <td>0.056478</td>\n",
       "      <td>0.024278</td>\n",
       "      <td>0.044103</td>\n",
       "      <td>0.178640</td>\n",
       "      <td>2.771002</td>\n",
       "      <td>39.913151</td>\n",
       "      <td>27.424742</td>\n",
       "      <td>315.746368</td>\n",
       "      <td>48.185921</td>\n",
       "      <td>21.574528</td>\n",
       "      <td>5.246092</td>\n",
       "      <td>90.772156</td>\n",
       "      <td>97.969963</td>\n",
       "      <td>9.265683</td>\n",
       "      <td>3.409077</td>\n",
       "      <td>0.427457</td>\n",
       "      <td>0.152426</td>\n",
       "      <td>0.393655</td>\n",
       "      <td>0.121764</td>\n",
       "      <td>0.365148</td>\n",
       "      <td>0.176663</td>\n",
       "      <td>0.401287</td>\n",
       "      <td>0.119749</td>\n",
       "      <td>0.391363</td>\n",
       "      <td>0.164670</td>\n",
       "      <td>0.359989</td>\n",
       "      <td>0.094411</td>\n",
       "      <td>0.236859</td>\n",
       "      <td>0.414591</td>\n",
       "      <td>0.155515</td>\n",
       "      <td>0.372589</td>\n",
       "      <td>0.165590</td>\n",
       "      <td>0.350717</td>\n",
       "      <td>0.110534</td>\n",
       "      <td>0.284084</td>\n",
       "      <td>0.344989</td>\n",
       "      <td>0.399181</td>\n",
       "      <td>0.332022</td>\n",
       "      <td>0.380127</td>\n",
       "      <td>0.269547</td>\n",
       "      <td>0.275511</td>\n",
       "      <td>0.261104</td>\n",
       "      <td>0.215609</td>\n",
       "      <td>0.224091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004427</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-36681.820312</td>\n",
       "      <td>-3.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.300000</td>\n",
       "      <td>-14.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.009207</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.008755</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>0.026456</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.012607</td>\n",
       "      <td>0.009896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542513</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-71.305000</td>\n",
       "      <td>-1.900000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.800000</td>\n",
       "      <td>-2.910000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.065000</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.018584</td>\n",
       "      <td>0.004951</td>\n",
       "      <td>0.025082</td>\n",
       "      <td>0.010681</td>\n",
       "      <td>0.079432</td>\n",
       "      <td>0.025406</td>\n",
       "      <td>0.011926</td>\n",
       "      <td>0.031449</td>\n",
       "      <td>0.062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.681239</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>30.200001</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.405000</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.005083</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.042358</td>\n",
       "      <td>0.009029</td>\n",
       "      <td>0.056524</td>\n",
       "      <td>0.017131</td>\n",
       "      <td>0.134583</td>\n",
       "      <td>0.098254</td>\n",
       "      <td>0.130911</td>\n",
       "      <td>0.046637</td>\n",
       "      <td>0.242732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.009124</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>43.430000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.014353</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.084359</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>0.095996</td>\n",
       "      <td>0.024981</td>\n",
       "      <td>0.177392</td>\n",
       "      <td>0.273595</td>\n",
       "      <td>0.297649</td>\n",
       "      <td>0.058233</td>\n",
       "      <td>0.399107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>69.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.156734</td>\n",
       "      <td>0.022453</td>\n",
       "      <td>0.017094</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>55.400002</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>5.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.520000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>7.710000</td>\n",
       "      <td>0.008430</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.054988</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.208568</td>\n",
       "      <td>0.037428</td>\n",
       "      <td>0.208174</td>\n",
       "      <td>0.047531</td>\n",
       "      <td>0.253881</td>\n",
       "      <td>0.694204</td>\n",
       "      <td>0.500788</td>\n",
       "      <td>0.079083</td>\n",
       "      <td>0.563130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30%</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.763589</td>\n",
       "      <td>0.163743</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.022263</td>\n",
       "      <td>1.230769</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>62.480000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>6.560000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>5.760000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.190000</td>\n",
       "      <td>0.028082</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.142352</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>0.011519</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>0.082063</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.010308</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.406364</td>\n",
       "      <td>0.089922</td>\n",
       "      <td>0.349834</td>\n",
       "      <td>0.090686</td>\n",
       "      <td>0.333780</td>\n",
       "      <td>0.869531</td>\n",
       "      <td>0.624665</td>\n",
       "      <td>0.100994</td>\n",
       "      <td>0.680337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40%</th>\n",
       "      <td>137.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.773862</td>\n",
       "      <td>0.169056</td>\n",
       "      <td>0.032967</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>67.889999</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>8.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.720000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.620000</td>\n",
       "      <td>0.086442</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.003090</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.004417</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.305504</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.009863</td>\n",
       "      <td>0.003281</td>\n",
       "      <td>0.034941</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.239454</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.009246</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.615134</td>\n",
       "      <td>0.221613</td>\n",
       "      <td>0.523326</td>\n",
       "      <td>0.179076</td>\n",
       "      <td>0.423257</td>\n",
       "      <td>0.927623</td>\n",
       "      <td>0.722531</td>\n",
       "      <td>0.128660</td>\n",
       "      <td>0.767424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>183.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.781746</td>\n",
       "      <td>0.173755</td>\n",
       "      <td>0.038321</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>13.250000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>73.070000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>8.670000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.300000</td>\n",
       "      <td>7.590000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.080000</td>\n",
       "      <td>0.320264</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.018464</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.026246</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.548668</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.031578</td>\n",
       "      <td>0.005581</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.495981</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.023920</td>\n",
       "      <td>0.004111</td>\n",
       "      <td>0.053539</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.785544</td>\n",
       "      <td>0.484208</td>\n",
       "      <td>0.682229</td>\n",
       "      <td>0.369254</td>\n",
       "      <td>0.531934</td>\n",
       "      <td>0.950007</td>\n",
       "      <td>0.803164</td>\n",
       "      <td>0.161804</td>\n",
       "      <td>0.830110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60%</th>\n",
       "      <td>248.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>0.178156</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.038282</td>\n",
       "      <td>1.379310</td>\n",
       "      <td>15.812500</td>\n",
       "      <td>11.333333</td>\n",
       "      <td>77.570000</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>9.880000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>8.440000</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>9.570000</td>\n",
       "      <td>0.708530</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>0.112352</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.121886</td>\n",
       "      <td>0.005517</td>\n",
       "      <td>0.795072</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.115324</td>\n",
       "      <td>0.009671</td>\n",
       "      <td>0.221977</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.007828</td>\n",
       "      <td>0.783632</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>0.072229</td>\n",
       "      <td>0.006877</td>\n",
       "      <td>0.136277</td>\n",
       "      <td>0.002420</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>0.894822</td>\n",
       "      <td>0.761336</td>\n",
       "      <td>0.813256</td>\n",
       "      <td>0.619739</td>\n",
       "      <td>0.645622</td>\n",
       "      <td>0.962009</td>\n",
       "      <td>0.862537</td>\n",
       "      <td>0.209047</td>\n",
       "      <td>0.874343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70%</th>\n",
       "      <td>347.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795812</td>\n",
       "      <td>0.182609</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>82.989998</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>11.270000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>9.380000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.150000</td>\n",
       "      <td>0.907262</td>\n",
       "      <td>0.013504</td>\n",
       "      <td>0.477728</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.386255</td>\n",
       "      <td>0.011124</td>\n",
       "      <td>0.942353</td>\n",
       "      <td>0.003586</td>\n",
       "      <td>0.448215</td>\n",
       "      <td>0.019154</td>\n",
       "      <td>0.485466</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.023076</td>\n",
       "      <td>0.937126</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.273951</td>\n",
       "      <td>0.013524</td>\n",
       "      <td>0.336987</td>\n",
       "      <td>0.005108</td>\n",
       "      <td>0.027056</td>\n",
       "      <td>0.946348</td>\n",
       "      <td>0.907302</td>\n",
       "      <td>0.900787</td>\n",
       "      <td>0.824164</td>\n",
       "      <td>0.759429</td>\n",
       "      <td>0.969272</td>\n",
       "      <td>0.908305</td>\n",
       "      <td>0.281552</td>\n",
       "      <td>0.902206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>513.000000</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.803859</td>\n",
       "      <td>0.188119</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.064892</td>\n",
       "      <td>1.489796</td>\n",
       "      <td>23.090910</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>89.080002</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>10.560000</td>\n",
       "      <td>13.250000</td>\n",
       "      <td>10.970000</td>\n",
       "      <td>0.977623</td>\n",
       "      <td>0.058326</td>\n",
       "      <td>0.856789</td>\n",
       "      <td>0.004473</td>\n",
       "      <td>0.757425</td>\n",
       "      <td>0.025517</td>\n",
       "      <td>0.985898</td>\n",
       "      <td>0.024098</td>\n",
       "      <td>0.868199</td>\n",
       "      <td>0.044761</td>\n",
       "      <td>0.773674</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>0.106731</td>\n",
       "      <td>0.983817</td>\n",
       "      <td>0.037125</td>\n",
       "      <td>0.751274</td>\n",
       "      <td>0.033228</td>\n",
       "      <td>0.680416</td>\n",
       "      <td>0.012342</td>\n",
       "      <td>0.177756</td>\n",
       "      <td>0.963967</td>\n",
       "      <td>0.956062</td>\n",
       "      <td>0.943553</td>\n",
       "      <td>0.927455</td>\n",
       "      <td>0.841520</td>\n",
       "      <td>0.974358</td>\n",
       "      <td>0.943308</td>\n",
       "      <td>0.388480</td>\n",
       "      <td>0.922978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>906.000000</td>\n",
       "      <td>0.025702</td>\n",
       "      <td>0.817204</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.085366</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>1.579515</td>\n",
       "      <td>30.200001</td>\n",
       "      <td>20.916666</td>\n",
       "      <td>97.699997</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>15.920000</td>\n",
       "      <td>11.900000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>12.250000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>12.460000</td>\n",
       "      <td>0.996515</td>\n",
       "      <td>0.268202</td>\n",
       "      <td>0.975014</td>\n",
       "      <td>0.017664</td>\n",
       "      <td>0.921648</td>\n",
       "      <td>0.150076</td>\n",
       "      <td>0.995555</td>\n",
       "      <td>0.162627</td>\n",
       "      <td>0.968837</td>\n",
       "      <td>0.156155</td>\n",
       "      <td>0.921710</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.457702</td>\n",
       "      <td>0.995457</td>\n",
       "      <td>0.209577</td>\n",
       "      <td>0.960419</td>\n",
       "      <td>0.126051</td>\n",
       "      <td>0.920417</td>\n",
       "      <td>0.036293</td>\n",
       "      <td>0.701740</td>\n",
       "      <td>0.970891</td>\n",
       "      <td>0.970775</td>\n",
       "      <td>0.963167</td>\n",
       "      <td>0.963691</td>\n",
       "      <td>0.899805</td>\n",
       "      <td>0.978561</td>\n",
       "      <td>0.966991</td>\n",
       "      <td>0.585700</td>\n",
       "      <td>0.940422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>1515.000000</td>\n",
       "      <td>0.056315</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.670986</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>38.333332</td>\n",
       "      <td>26.500000</td>\n",
       "      <td>105.150002</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>19.745000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>23.200001</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>14.460000</td>\n",
       "      <td>0.997980</td>\n",
       "      <td>0.454555</td>\n",
       "      <td>0.988256</td>\n",
       "      <td>0.095150</td>\n",
       "      <td>0.959286</td>\n",
       "      <td>0.530964</td>\n",
       "      <td>0.997122</td>\n",
       "      <td>0.341257</td>\n",
       "      <td>0.983510</td>\n",
       "      <td>0.490485</td>\n",
       "      <td>0.954084</td>\n",
       "      <td>0.027045</td>\n",
       "      <td>0.777708</td>\n",
       "      <td>0.997670</td>\n",
       "      <td>0.457312</td>\n",
       "      <td>0.985532</td>\n",
       "      <td>0.383906</td>\n",
       "      <td>0.970206</td>\n",
       "      <td>0.084218</td>\n",
       "      <td>0.908421</td>\n",
       "      <td>0.973593</td>\n",
       "      <td>0.974590</td>\n",
       "      <td>0.969417</td>\n",
       "      <td>0.972049</td>\n",
       "      <td>0.923598</td>\n",
       "      <td>0.980474</td>\n",
       "      <td>0.975058</td>\n",
       "      <td>0.748471</td>\n",
       "      <td>0.949683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>4265.000000</td>\n",
       "      <td>0.171652</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.223081</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.820256</td>\n",
       "      <td>1.998403</td>\n",
       "      <td>94.500000</td>\n",
       "      <td>67.375000</td>\n",
       "      <td>117.160004</td>\n",
       "      <td>54.150000</td>\n",
       "      <td>54.205002</td>\n",
       "      <td>15.900000</td>\n",
       "      <td>72.650002</td>\n",
       "      <td>21.885000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>19.920000</td>\n",
       "      <td>0.998784</td>\n",
       "      <td>0.693852</td>\n",
       "      <td>0.993654</td>\n",
       "      <td>0.767351</td>\n",
       "      <td>0.979325</td>\n",
       "      <td>0.862270</td>\n",
       "      <td>0.998289</td>\n",
       "      <td>0.573691</td>\n",
       "      <td>0.991901</td>\n",
       "      <td>0.833779</td>\n",
       "      <td>0.986330</td>\n",
       "      <td>0.625598</td>\n",
       "      <td>0.960058</td>\n",
       "      <td>0.999007</td>\n",
       "      <td>0.751283</td>\n",
       "      <td>0.994593</td>\n",
       "      <td>0.907073</td>\n",
       "      <td>0.989867</td>\n",
       "      <td>0.748681</td>\n",
       "      <td>0.978481</td>\n",
       "      <td>0.976650</td>\n",
       "      <td>0.977521</td>\n",
       "      <td>0.975070</td>\n",
       "      <td>0.978209</td>\n",
       "      <td>0.943242</td>\n",
       "      <td>0.982971</td>\n",
       "      <td>0.979994</td>\n",
       "      <td>0.908731</td>\n",
       "      <td>0.959673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5155.000000</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.999042</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.994366</td>\n",
       "      <td>0.995930</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>943.500000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>120.209999</td>\n",
       "      <td>5130.399902</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>27.900000</td>\n",
       "      <td>7775.100098</td>\n",
       "      <td>9517.349609</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>62.040001</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>0.914266</td>\n",
       "      <td>0.996912</td>\n",
       "      <td>0.931933</td>\n",
       "      <td>0.989254</td>\n",
       "      <td>0.960253</td>\n",
       "      <td>0.999365</td>\n",
       "      <td>0.793751</td>\n",
       "      <td>0.995735</td>\n",
       "      <td>0.965330</td>\n",
       "      <td>0.995287</td>\n",
       "      <td>0.953006</td>\n",
       "      <td>0.991790</td>\n",
       "      <td>0.999782</td>\n",
       "      <td>0.954891</td>\n",
       "      <td>0.997884</td>\n",
       "      <td>0.992897</td>\n",
       "      <td>0.997954</td>\n",
       "      <td>0.974670</td>\n",
       "      <td>0.994662</td>\n",
       "      <td>0.979981</td>\n",
       "      <td>0.980329</td>\n",
       "      <td>0.980767</td>\n",
       "      <td>0.982711</td>\n",
       "      <td>0.958438</td>\n",
       "      <td>0.986617</td>\n",
       "      <td>0.983883</td>\n",
       "      <td>0.989838</td>\n",
       "      <td>0.967559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length    digit_frac   letter_frac    space_frac     punc_frac  \\\n",
       "count  14251.000000  14251.000000  14251.000000  14251.000000  14251.000000   \n",
       "mean     407.412111      0.009792      0.771801      0.171560      0.046848   \n",
       "std      687.677043      0.032306      0.056478      0.024278      0.044103   \n",
       "min        8.000000      0.000000      0.004427      0.000403      0.000000   \n",
       "1%        22.000000      0.000000      0.542513      0.095238      0.000000   \n",
       "5%        31.000000      0.000000      0.681239      0.130435      0.000000   \n",
       "10%       43.000000      0.000000      0.721519      0.142857      0.014085   \n",
       "20%       69.000000      0.000000      0.750000      0.156734      0.022453   \n",
       "30%       99.000000      0.000000      0.763589      0.163743      0.027778   \n",
       "40%      137.000000      0.000000      0.773862      0.169056      0.032967   \n",
       "50%      183.000000      0.000000      0.781746      0.173755      0.038321   \n",
       "60%      248.000000      0.000000      0.788732      0.178156      0.044444   \n",
       "70%      347.000000      0.000000      0.795812      0.182609      0.051724   \n",
       "80%      513.000000      0.005882      0.803859      0.188119      0.062500   \n",
       "90%      906.000000      0.025702      0.817204      0.196970      0.085366   \n",
       "95%     1515.000000      0.056315      0.833333      0.205128      0.111111   \n",
       "99%     4265.000000      0.171652      0.869565      0.223081      0.190476   \n",
       "max     5155.000000      0.476190      0.999042      0.480000      0.994366   \n",
       "\n",
       "         upper_frac  syllables_per_word  syllables_per_sent  words_per_sent  \\\n",
       "count  14251.000000        14251.000000        14251.000000    14251.000000   \n",
       "mean       0.087480            1.366432           18.896488       13.300020   \n",
       "std        0.178640            2.771002           39.913151       27.424742   \n",
       "min        0.000000            0.666667            1.000000        1.000000   \n",
       "1%         0.000000            0.857143            2.500000        2.000000   \n",
       "5%         0.000000            1.000000            3.500000        2.666667   \n",
       "10%        0.009124            1.071429            4.666667        3.500000   \n",
       "20%        0.017094            1.166667            7.000000        5.000000   \n",
       "30%        0.022263            1.230769            9.000000        6.500000   \n",
       "40%        0.027027            1.285714           11.000000        8.000000   \n",
       "50%        0.032000            1.333333           13.250000        9.500000   \n",
       "60%        0.038282            1.379310           15.812500       11.333333   \n",
       "70%        0.047170            1.428571           19.000000       13.500000   \n",
       "80%        0.064892            1.489796           23.090910       16.250000   \n",
       "90%        0.150000            1.579515           30.200001       20.916666   \n",
       "95%        0.670986            1.666667           38.333332       26.500000   \n",
       "99%        0.820256            1.998403           94.500000       67.375000   \n",
       "max        0.995930          327.000000          943.500000      625.000000   \n",
       "\n",
       "       flesch_reading_ease  flesch_kincaid_grade   gunning_fog    smog_index  \\\n",
       "count         14251.000000          14251.000000  14251.000000  14251.000000   \n",
       "mean             63.766296              9.520686     11.068330      4.353828   \n",
       "std             315.746368             48.185921     21.574528      5.246092   \n",
       "min          -36681.820312             -3.100000      0.800000      0.000000   \n",
       "1%              -71.305000             -1.900000      1.600000      0.000000   \n",
       "5%               30.200001              0.500000      2.400000      0.000000   \n",
       "10%              43.430000              1.800000      3.200000      0.000000   \n",
       "20%              55.400002              3.400000      5.010000      0.000000   \n",
       "30%              62.480000              4.500000      6.560000      0.000000   \n",
       "40%              67.889999              5.600000      8.010000      0.000000   \n",
       "50%              73.070000              6.800000      8.670000      0.000000   \n",
       "60%              77.570000              7.800000      9.880000      6.400000   \n",
       "70%              82.989998              9.000000     11.270000      8.800000   \n",
       "80%              89.080002             10.700000     12.750000     10.100000   \n",
       "90%              97.699997             13.800000     15.920000     11.900000   \n",
       "95%             105.150002             17.900000     19.745000     13.000000   \n",
       "99%             117.160004             54.150000     54.205002     15.900000   \n",
       "max             120.209999           5130.399902    500.000000     27.900000   \n",
       "\n",
       "       automated_readability_index  coleman_liau_index  linsear_write_formula  \\\n",
       "count                 14251.000000        14251.000000           14251.000000   \n",
       "mean                     13.135674            9.296756               9.612117   \n",
       "std                      90.772156           97.969963               9.265683   \n",
       "min                      -9.300000          -14.150000               0.000000   \n",
       "1%                       -2.800000           -2.910000               1.000000   \n",
       "5%                        0.600000            0.590000               2.000000   \n",
       "10%                       2.300000            2.600000               2.500000   \n",
       "20%                       4.300000            4.520000               3.800000   \n",
       "30%                       5.800000            5.760000               5.000000   \n",
       "40%                       7.000000            6.720000               6.000000   \n",
       "50%                       8.300000            7.590000               7.000000   \n",
       "60%                       9.600000            8.440000               8.333333   \n",
       "70%                      11.100000            9.380000              11.000000   \n",
       "80%                      13.300000           10.560000              13.250000   \n",
       "90%                      17.400000           12.250000              18.000000   \n",
       "95%                      23.200001           14.100000              24.500000   \n",
       "99%                      72.650002           21.885000              54.000000   \n",
       "max                    7775.100098         9517.349609             150.000000   \n",
       "\n",
       "       dale_chall_readability_score  dto_toxicity  dto_severe_toxicity  \\\n",
       "count                  14251.000000  14251.000000         14251.000000   \n",
       "mean                       9.582970      0.453714             0.065993   \n",
       "std                        3.409077      0.427457             0.152426   \n",
       "min                        0.100000      0.000530             0.000079   \n",
       "1%                         1.065000      0.000656             0.000087   \n",
       "5%                         6.405000      0.000928             0.000093   \n",
       "10%                        7.000000      0.001703             0.000100   \n",
       "20%                        7.710000      0.008430             0.000113   \n",
       "30%                        8.190000      0.028082             0.000148   \n",
       "40%                        8.620000      0.086442             0.000290   \n",
       "50%                        9.080000      0.320264             0.000867   \n",
       "60%                        9.570000      0.708530             0.003229   \n",
       "70%                       10.150000      0.907262             0.013504   \n",
       "80%                       10.970000      0.977623             0.058326   \n",
       "90%                       12.460000      0.996515             0.268202   \n",
       "95%                       14.460000      0.997980             0.454555   \n",
       "99%                       19.920000      0.998784             0.693852   \n",
       "max                       62.040001      0.999275             0.914266   \n",
       "\n",
       "        dto_obscene    dto_threat    dto_insult  dto_identity_attack  \\\n",
       "count  14251.000000  14251.000000  14251.000000         14251.000000   \n",
       "mean       0.293632      0.028148      0.271550             0.062582   \n",
       "std        0.393655      0.121764      0.365148             0.176663   \n",
       "min        0.000152      0.000089      0.000164             0.000127   \n",
       "1%         0.000165      0.000099      0.000175             0.000136   \n",
       "5%         0.000178      0.000109      0.000185             0.000144   \n",
       "10%        0.000206      0.000118      0.000212             0.000160   \n",
       "20%        0.000448      0.000158      0.000485             0.000249   \n",
       "30%        0.000999      0.000274      0.001306             0.000458   \n",
       "40%        0.003090      0.000485      0.004417             0.001057   \n",
       "50%        0.018464      0.000855      0.026246             0.002499   \n",
       "60%        0.112352      0.001319      0.121886             0.005517   \n",
       "70%        0.477728      0.002247      0.386255             0.011124   \n",
       "80%        0.856789      0.004473      0.757425             0.025517   \n",
       "90%        0.975014      0.017664      0.921648             0.150076   \n",
       "95%        0.988256      0.095150      0.959286             0.530964   \n",
       "99%        0.993654      0.767351      0.979325             0.862270   \n",
       "max        0.996912      0.931933      0.989254             0.960253   \n",
       "\n",
       "       dtu_toxicity  dtu_severe_toxicity   dtu_obscene  dtu_identity_attack  \\\n",
       "count  14251.000000         14251.000000  14251.000000         14251.000000   \n",
       "mean       0.528137             0.045724      0.293552             0.064130   \n",
       "std        0.401287             0.119749      0.391363             0.164670   \n",
       "min        0.000352             0.000001      0.000017             0.000060   \n",
       "1%         0.001169             0.000002      0.000068             0.000125   \n",
       "5%         0.005083             0.000007      0.000246             0.000318   \n",
       "10%        0.014353             0.000012      0.000495             0.000546   \n",
       "20%        0.054988             0.000027      0.001367             0.001093   \n",
       "30%        0.142352             0.000058      0.003603             0.001903   \n",
       "40%        0.305504             0.000121      0.009863             0.003281   \n",
       "50%        0.548668             0.000286      0.031578             0.005581   \n",
       "60%        0.795072             0.000787      0.115324             0.009671   \n",
       "70%        0.942353             0.003586      0.448215             0.019154   \n",
       "80%        0.985898             0.024098      0.868199             0.044761   \n",
       "90%        0.995555             0.162627      0.968837             0.156155   \n",
       "95%        0.997122             0.341257      0.983510             0.490485   \n",
       "99%        0.998289             0.573691      0.991901             0.833779   \n",
       "max        0.999365             0.793751      0.995735             0.965330   \n",
       "\n",
       "         dtu_insult    dtu_threat  dtu_sexual_explicit  dtm_toxicity  \\\n",
       "count  14251.000000  14251.000000         14251.000000  14251.000000   \n",
       "mean       0.303063      0.017880             0.108263      0.503549   \n",
       "std        0.359989      0.094411             0.236859      0.414591   \n",
       "min        0.000061      0.000015             0.000010      0.000126   \n",
       "1%         0.000101      0.000032             0.000035      0.000399   \n",
       "5%         0.000271      0.000062             0.000091      0.001026   \n",
       "10%        0.000815      0.000094             0.000169      0.003111   \n",
       "20%        0.003580      0.000165             0.000404      0.019481   \n",
       "30%        0.011519      0.000260             0.000849      0.082063   \n",
       "40%        0.034941      0.000394             0.001747      0.239454   \n",
       "50%        0.094486      0.000609             0.003564      0.495981   \n",
       "60%        0.221977      0.000980             0.007828      0.783632   \n",
       "70%        0.485466      0.001654             0.023076      0.937126   \n",
       "80%        0.773674      0.003020             0.106731      0.983817   \n",
       "90%        0.921710      0.008064             0.457702      0.995457   \n",
       "95%        0.954084      0.027045             0.777708      0.997670   \n",
       "99%        0.986330      0.625598             0.960058      0.999007   \n",
       "max        0.995287      0.953006             0.991790      0.999782   \n",
       "\n",
       "       dtm_severe_toxicity   dtm_obscene  dtm_identity_attack    dtm_insult  \\\n",
       "count         14251.000000  14251.000000         14251.000000  14251.000000   \n",
       "mean              0.060730      0.259472             0.057912      0.266497   \n",
       "std               0.155515      0.372589             0.165590      0.350717   \n",
       "min               0.000011      0.000052             0.000068      0.000128   \n",
       "1%                0.000016      0.000085             0.000108      0.000182   \n",
       "5%                0.000023      0.000146             0.000172      0.000357   \n",
       "10%               0.000035      0.000310             0.000278      0.000856   \n",
       "20%               0.000099      0.001277             0.000691      0.003652   \n",
       "30%               0.000260      0.003600             0.001383      0.010308   \n",
       "40%               0.000620      0.009246             0.002448      0.023793   \n",
       "50%               0.001396      0.023920             0.004111      0.053539   \n",
       "60%               0.003218      0.072229             0.006877      0.136277   \n",
       "70%               0.009112      0.273951             0.013524      0.336987   \n",
       "80%               0.037125      0.751274             0.033228      0.680416   \n",
       "90%               0.209577      0.960419             0.126051      0.920417   \n",
       "95%               0.457312      0.985532             0.383906      0.970206   \n",
       "99%               0.751283      0.994593             0.907073      0.989867   \n",
       "max               0.954891      0.997884             0.992897      0.997954   \n",
       "\n",
       "         dtm_threat  dtm_sexual_explicit   hb_bert_off   hb_bert_abu  \\\n",
       "count  14251.000000         14251.000000  14251.000000  14251.000000   \n",
       "mean       0.026968             0.141305      0.637523      0.493512   \n",
       "std        0.110534             0.284084      0.344989      0.399181   \n",
       "min        0.000017             0.000013      0.009207      0.002739   \n",
       "1%         0.000027             0.000018      0.018584      0.004951   \n",
       "5%         0.000037             0.000024      0.042358      0.009029   \n",
       "10%        0.000055             0.000037      0.084359      0.015033   \n",
       "20%        0.000154             0.000128      0.208568      0.037428   \n",
       "30%        0.000368             0.000365      0.406364      0.089922   \n",
       "40%        0.000716             0.000862      0.615134      0.221613   \n",
       "50%        0.001302             0.002158      0.785544      0.484208   \n",
       "60%        0.002420             0.006268      0.894822      0.761336   \n",
       "70%        0.005108             0.027056      0.946348      0.907302   \n",
       "80%        0.012342             0.177756      0.963967      0.956062   \n",
       "90%        0.036293             0.701740      0.970891      0.970775   \n",
       "95%        0.084218             0.908421      0.973593      0.974590   \n",
       "99%        0.748681             0.978481      0.976650      0.977521   \n",
       "max        0.974670             0.994662      0.979981      0.980329   \n",
       "\n",
       "       hb_hatebert_off  hb_hatebert_abu  te_roberta_off  te_roberta_emo_anger  \\\n",
       "count     14251.000000     14251.000000    14251.000000          14251.000000   \n",
       "mean          0.598392         0.453566        0.538566              0.810702   \n",
       "std           0.332022         0.380127        0.269547              0.275511   \n",
       "min           0.008755         0.005975        0.026456              0.005610   \n",
       "1%            0.025082         0.010681        0.079432              0.025406   \n",
       "5%            0.056524         0.017131        0.134583              0.098254   \n",
       "10%           0.095996         0.024981        0.177392              0.273595   \n",
       "20%           0.208174         0.047531        0.253881              0.694204   \n",
       "30%           0.349834         0.090686        0.333780              0.869531   \n",
       "40%           0.523326         0.179076        0.423257              0.927623   \n",
       "50%           0.682229         0.369254        0.531934              0.950007   \n",
       "60%           0.813256         0.619739        0.645622              0.962009   \n",
       "70%           0.900787         0.824164        0.759429              0.969272   \n",
       "80%           0.943553         0.927455        0.841520              0.974358   \n",
       "90%           0.963167         0.963691        0.899805              0.978561   \n",
       "95%           0.969417         0.972049        0.923598              0.980474   \n",
       "99%           0.975070         0.978209        0.943242              0.982971   \n",
       "max           0.980767         0.982711        0.958438              0.986617   \n",
       "\n",
       "       te_roberta_snt_neg  te_roberta_iro  te_xlm_roberta_snt_neg  \n",
       "count        14251.000000    14251.000000            14251.000000  \n",
       "mean             0.713403        0.245387                0.740695  \n",
       "std              0.261104        0.215609                0.224091  \n",
       "min              0.001072        0.012607                0.009896  \n",
       "1%               0.011926        0.031449                0.062300  \n",
       "5%               0.130911        0.046637                0.242732  \n",
       "10%              0.297649        0.058233                0.399107  \n",
       "20%              0.500788        0.079083                0.563130  \n",
       "30%              0.624665        0.100994                0.680337  \n",
       "40%              0.722531        0.128660                0.767424  \n",
       "50%              0.803164        0.161804                0.830110  \n",
       "60%              0.862537        0.209047                0.874343  \n",
       "70%              0.908305        0.281552                0.902206  \n",
       "80%              0.943308        0.388480                0.922978  \n",
       "90%              0.966991        0.585700                0.940422  \n",
       "95%              0.975058        0.748471                0.949683  \n",
       "99%              0.979994        0.908731                0.959673  \n",
       "max              0.983883        0.989838                0.967559  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"length\"]\n",
    "cols += list(char_fns.keys())\n",
    "cols += list(textstat_fns.keys())\n",
    "cols += dtfy_fs\n",
    "cols += list(conf.hatebert_models.keys())\n",
    "cols += list(conf.tweeteval_models.keys())\n",
    "df[cols].describe(percentiles=percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bddf8914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14251 entries, 0 to 14250\n",
      "Data columns (total 431 columns):\n",
      " #    Column                        Non-Null Count  Dtype  \n",
      "---   ------                        --------------  -----  \n",
      " 0    text                          14251 non-null  object \n",
      " 1    length                        14251 non-null  int16  \n",
      " 2    digit_frac                    14251 non-null  float32\n",
      " 3    letter_frac                   14251 non-null  float32\n",
      " 4    space_frac                    14251 non-null  float32\n",
      " 5    punc_frac                     14251 non-null  float32\n",
      " 6    upper_frac                    14251 non-null  float32\n",
      " 7    syllables_per_word            14251 non-null  float32\n",
      " 8    syllables_per_sent            14251 non-null  float32\n",
      " 9    words_per_sent                14251 non-null  float32\n",
      " 10   flesch_reading_ease           14251 non-null  float32\n",
      " 11   flesch_kincaid_grade          14251 non-null  float32\n",
      " 12   gunning_fog                   14251 non-null  float32\n",
      " 13   smog_index                    14251 non-null  float32\n",
      " 14   automated_readability_index   14251 non-null  float32\n",
      " 15   coleman_liau_index            14251 non-null  float32\n",
      " 16   linsear_write_formula         14251 non-null  float32\n",
      " 17   dale_chall_readability_score  14251 non-null  float32\n",
      " 18   dto_toxicity                  14251 non-null  float32\n",
      " 19   dto_severe_toxicity           14251 non-null  float32\n",
      " 20   dto_obscene                   14251 non-null  float32\n",
      " 21   dto_threat                    14251 non-null  float32\n",
      " 22   dto_insult                    14251 non-null  float32\n",
      " 23   dto_identity_attack           14251 non-null  float32\n",
      " 24   dtu_toxicity                  14251 non-null  float32\n",
      " 25   dtu_severe_toxicity           14251 non-null  float32\n",
      " 26   dtu_obscene                   14251 non-null  float32\n",
      " 27   dtu_identity_attack           14251 non-null  float32\n",
      " 28   dtu_insult                    14251 non-null  float32\n",
      " 29   dtu_threat                    14251 non-null  float32\n",
      " 30   dtu_sexual_explicit           14251 non-null  float32\n",
      " 31   dtm_toxicity                  14251 non-null  float32\n",
      " 32   dtm_severe_toxicity           14251 non-null  float32\n",
      " 33   dtm_obscene                   14251 non-null  float32\n",
      " 34   dtm_identity_attack           14251 non-null  float32\n",
      " 35   dtm_insult                    14251 non-null  float32\n",
      " 36   dtm_threat                    14251 non-null  float32\n",
      " 37   dtm_sexual_explicit           14251 non-null  float32\n",
      " 38   hb_bert_off                   14251 non-null  float32\n",
      " 39   hb_bert_abu                   14251 non-null  float32\n",
      " 40   hb_hatebert_off               14251 non-null  float32\n",
      " 41   hb_hatebert_abu               14251 non-null  float32\n",
      " 42   te_roberta_off                14251 non-null  float32\n",
      " 43   te_roberta_emo_anger          14251 non-null  float32\n",
      " 44   te_roberta_snt_neg            14251 non-null  float32\n",
      " 45   te_roberta_iro                14251 non-null  float32\n",
      " 46   te_xlm_roberta_snt_neg        14251 non-null  float32\n",
      " 47   zz0000                        14251 non-null  float32\n",
      " 48   zz0001                        14251 non-null  float32\n",
      " 49   zz0002                        14251 non-null  float32\n",
      " 50   zz0003                        14251 non-null  float32\n",
      " 51   zz0004                        14251 non-null  float32\n",
      " 52   zz0005                        14251 non-null  float32\n",
      " 53   zz0006                        14251 non-null  float32\n",
      " 54   zz0007                        14251 non-null  float32\n",
      " 55   zz0008                        14251 non-null  float32\n",
      " 56   zz0009                        14251 non-null  float32\n",
      " 57   zz0010                        14251 non-null  float32\n",
      " 58   zz0011                        14251 non-null  float32\n",
      " 59   zz0012                        14251 non-null  float32\n",
      " 60   zz0013                        14251 non-null  float32\n",
      " 61   zz0014                        14251 non-null  float32\n",
      " 62   zz0015                        14251 non-null  float32\n",
      " 63   zz0016                        14251 non-null  float32\n",
      " 64   zz0017                        14251 non-null  float32\n",
      " 65   zz0018                        14251 non-null  float32\n",
      " 66   zz0019                        14251 non-null  float32\n",
      " 67   zz0020                        14251 non-null  float32\n",
      " 68   zz0021                        14251 non-null  float32\n",
      " 69   zz0022                        14251 non-null  float32\n",
      " 70   zz0023                        14251 non-null  float32\n",
      " 71   zz0024                        14251 non-null  float32\n",
      " 72   zz0025                        14251 non-null  float32\n",
      " 73   zz0026                        14251 non-null  float32\n",
      " 74   zz0027                        14251 non-null  float32\n",
      " 75   zz0028                        14251 non-null  float32\n",
      " 76   zz0029                        14251 non-null  float32\n",
      " 77   zz0030                        14251 non-null  float32\n",
      " 78   zz0031                        14251 non-null  float32\n",
      " 79   zz0032                        14251 non-null  float32\n",
      " 80   zz0033                        14251 non-null  float32\n",
      " 81   zz0034                        14251 non-null  float32\n",
      " 82   zz0035                        14251 non-null  float32\n",
      " 83   zz0036                        14251 non-null  float32\n",
      " 84   zz0037                        14251 non-null  float32\n",
      " 85   zz0038                        14251 non-null  float32\n",
      " 86   zz0039                        14251 non-null  float32\n",
      " 87   zz0040                        14251 non-null  float32\n",
      " 88   zz0041                        14251 non-null  float32\n",
      " 89   zz0042                        14251 non-null  float32\n",
      " 90   zz0043                        14251 non-null  float32\n",
      " 91   zz0044                        14251 non-null  float32\n",
      " 92   zz0045                        14251 non-null  float32\n",
      " 93   zz0046                        14251 non-null  float32\n",
      " 94   zz0047                        14251 non-null  float32\n",
      " 95   zz0048                        14251 non-null  float32\n",
      " 96   zz0049                        14251 non-null  float32\n",
      " 97   zz0050                        14251 non-null  float32\n",
      " 98   zz0051                        14251 non-null  float32\n",
      " 99   zz0052                        14251 non-null  float32\n",
      " 100  zz0053                        14251 non-null  float32\n",
      " 101  zz0054                        14251 non-null  float32\n",
      " 102  zz0055                        14251 non-null  float32\n",
      " 103  zz0056                        14251 non-null  float32\n",
      " 104  zz0057                        14251 non-null  float32\n",
      " 105  zz0058                        14251 non-null  float32\n",
      " 106  zz0059                        14251 non-null  float32\n",
      " 107  zz0060                        14251 non-null  float32\n",
      " 108  zz0061                        14251 non-null  float32\n",
      " 109  zz0062                        14251 non-null  float32\n",
      " 110  zz0063                        14251 non-null  float32\n",
      " 111  zz0064                        14251 non-null  float32\n",
      " 112  zz0065                        14251 non-null  float32\n",
      " 113  zz0066                        14251 non-null  float32\n",
      " 114  zz0067                        14251 non-null  float32\n",
      " 115  zz0068                        14251 non-null  float32\n",
      " 116  zz0069                        14251 non-null  float32\n",
      " 117  zz0070                        14251 non-null  float32\n",
      " 118  zz0071                        14251 non-null  float32\n",
      " 119  zz0072                        14251 non-null  float32\n",
      " 120  zz0073                        14251 non-null  float32\n",
      " 121  zz0074                        14251 non-null  float32\n",
      " 122  zz0075                        14251 non-null  float32\n",
      " 123  zz0076                        14251 non-null  float32\n",
      " 124  zz0077                        14251 non-null  float32\n",
      " 125  zz0078                        14251 non-null  float32\n",
      " 126  zz0079                        14251 non-null  float32\n",
      " 127  zz0080                        14251 non-null  float32\n",
      " 128  zz0081                        14251 non-null  float32\n",
      " 129  zz0082                        14251 non-null  float32\n",
      " 130  zz0083                        14251 non-null  float32\n",
      " 131  zz0084                        14251 non-null  float32\n",
      " 132  zz0085                        14251 non-null  float32\n",
      " 133  zz0086                        14251 non-null  float32\n",
      " 134  zz0087                        14251 non-null  float32\n",
      " 135  zz0088                        14251 non-null  float32\n",
      " 136  zz0089                        14251 non-null  float32\n",
      " 137  zz0090                        14251 non-null  float32\n",
      " 138  zz0091                        14251 non-null  float32\n",
      " 139  zz0092                        14251 non-null  float32\n",
      " 140  zz0093                        14251 non-null  float32\n",
      " 141  zz0094                        14251 non-null  float32\n",
      " 142  zz0095                        14251 non-null  float32\n",
      " 143  zz0096                        14251 non-null  float32\n",
      " 144  zz0097                        14251 non-null  float32\n",
      " 145  zz0098                        14251 non-null  float32\n",
      " 146  zz0099                        14251 non-null  float32\n",
      " 147  zz0100                        14251 non-null  float32\n",
      " 148  zz0101                        14251 non-null  float32\n",
      " 149  zz0102                        14251 non-null  float32\n",
      " 150  zz0103                        14251 non-null  float32\n",
      " 151  zz0104                        14251 non-null  float32\n",
      " 152  zz0105                        14251 non-null  float32\n",
      " 153  zz0106                        14251 non-null  float32\n",
      " 154  zz0107                        14251 non-null  float32\n",
      " 155  zz0108                        14251 non-null  float32\n",
      " 156  zz0109                        14251 non-null  float32\n",
      " 157  zz0110                        14251 non-null  float32\n",
      " 158  zz0111                        14251 non-null  float32\n",
      " 159  zz0112                        14251 non-null  float32\n",
      " 160  zz0113                        14251 non-null  float32\n",
      " 161  zz0114                        14251 non-null  float32\n",
      " 162  zz0115                        14251 non-null  float32\n",
      " 163  zz0116                        14251 non-null  float32\n",
      " 164  zz0117                        14251 non-null  float32\n",
      " 165  zz0118                        14251 non-null  float32\n",
      " 166  zz0119                        14251 non-null  float32\n",
      " 167  zz0120                        14251 non-null  float32\n",
      " 168  zz0121                        14251 non-null  float32\n",
      " 169  zz0122                        14251 non-null  float32\n",
      " 170  zz0123                        14251 non-null  float32\n",
      " 171  zz0124                        14251 non-null  float32\n",
      " 172  zz0125                        14251 non-null  float32\n",
      " 173  zz0126                        14251 non-null  float32\n",
      " 174  zz0127                        14251 non-null  float32\n",
      " 175  zz0128                        14251 non-null  float32\n",
      " 176  zz0129                        14251 non-null  float32\n",
      " 177  zz0130                        14251 non-null  float32\n",
      " 178  zz0131                        14251 non-null  float32\n",
      " 179  zz0132                        14251 non-null  float32\n",
      " 180  zz0133                        14251 non-null  float32\n",
      " 181  zz0134                        14251 non-null  float32\n",
      " 182  zz0135                        14251 non-null  float32\n",
      " 183  zz0136                        14251 non-null  float32\n",
      " 184  zz0137                        14251 non-null  float32\n",
      " 185  zz0138                        14251 non-null  float32\n",
      " 186  zz0139                        14251 non-null  float32\n",
      " 187  zz0140                        14251 non-null  float32\n",
      " 188  zz0141                        14251 non-null  float32\n",
      " 189  zz0142                        14251 non-null  float32\n",
      " 190  zz0143                        14251 non-null  float32\n",
      " 191  zz0144                        14251 non-null  float32\n",
      " 192  zz0145                        14251 non-null  float32\n",
      " 193  zz0146                        14251 non-null  float32\n",
      " 194  zz0147                        14251 non-null  float32\n",
      " 195  zz0148                        14251 non-null  float32\n",
      " 196  zz0149                        14251 non-null  float32\n",
      " 197  zz0150                        14251 non-null  float32\n",
      " 198  zz0151                        14251 non-null  float32\n",
      " 199  zz0152                        14251 non-null  float32\n",
      " 200  zz0153                        14251 non-null  float32\n",
      " 201  zz0154                        14251 non-null  float32\n",
      " 202  zz0155                        14251 non-null  float32\n",
      " 203  zz0156                        14251 non-null  float32\n",
      " 204  zz0157                        14251 non-null  float32\n",
      " 205  zz0158                        14251 non-null  float32\n",
      " 206  zz0159                        14251 non-null  float32\n",
      " 207  zz0160                        14251 non-null  float32\n",
      " 208  zz0161                        14251 non-null  float32\n",
      " 209  zz0162                        14251 non-null  float32\n",
      " 210  zz0163                        14251 non-null  float32\n",
      " 211  zz0164                        14251 non-null  float32\n",
      " 212  zz0165                        14251 non-null  float32\n",
      " 213  zz0166                        14251 non-null  float32\n",
      " 214  zz0167                        14251 non-null  float32\n",
      " 215  zz0168                        14251 non-null  float32\n",
      " 216  zz0169                        14251 non-null  float32\n",
      " 217  zz0170                        14251 non-null  float32\n",
      " 218  zz0171                        14251 non-null  float32\n",
      " 219  zz0172                        14251 non-null  float32\n",
      " 220  zz0173                        14251 non-null  float32\n",
      " 221  zz0174                        14251 non-null  float32\n",
      " 222  zz0175                        14251 non-null  float32\n",
      " 223  zz0176                        14251 non-null  float32\n",
      " 224  zz0177                        14251 non-null  float32\n",
      " 225  zz0178                        14251 non-null  float32\n",
      " 226  zz0179                        14251 non-null  float32\n",
      " 227  zz0180                        14251 non-null  float32\n",
      " 228  zz0181                        14251 non-null  float32\n",
      " 229  zz0182                        14251 non-null  float32\n",
      " 230  zz0183                        14251 non-null  float32\n",
      " 231  zz0184                        14251 non-null  float32\n",
      " 232  zz0185                        14251 non-null  float32\n",
      " 233  zz0186                        14251 non-null  float32\n",
      " 234  zz0187                        14251 non-null  float32\n",
      " 235  zz0188                        14251 non-null  float32\n",
      " 236  zz0189                        14251 non-null  float32\n",
      " 237  zz0190                        14251 non-null  float32\n",
      " 238  zz0191                        14251 non-null  float32\n",
      " 239  zz0192                        14251 non-null  float32\n",
      " 240  zz0193                        14251 non-null  float32\n",
      " 241  zz0194                        14251 non-null  float32\n",
      " 242  zz0195                        14251 non-null  float32\n",
      " 243  zz0196                        14251 non-null  float32\n",
      " 244  zz0197                        14251 non-null  float32\n",
      " 245  zz0198                        14251 non-null  float32\n",
      " 246  zz0199                        14251 non-null  float32\n",
      " 247  zz0200                        14251 non-null  float32\n",
      " 248  zz0201                        14251 non-null  float32\n",
      " 249  zz0202                        14251 non-null  float32\n",
      " 250  zz0203                        14251 non-null  float32\n",
      " 251  zz0204                        14251 non-null  float32\n",
      " 252  zz0205                        14251 non-null  float32\n",
      " 253  zz0206                        14251 non-null  float32\n",
      " 254  zz0207                        14251 non-null  float32\n",
      " 255  zz0208                        14251 non-null  float32\n",
      " 256  zz0209                        14251 non-null  float32\n",
      " 257  zz0210                        14251 non-null  float32\n",
      " 258  zz0211                        14251 non-null  float32\n",
      " 259  zz0212                        14251 non-null  float32\n",
      " 260  zz0213                        14251 non-null  float32\n",
      " 261  zz0214                        14251 non-null  float32\n",
      " 262  zz0215                        14251 non-null  float32\n",
      " 263  zz0216                        14251 non-null  float32\n",
      " 264  zz0217                        14251 non-null  float32\n",
      " 265  zz0218                        14251 non-null  float32\n",
      " 266  zz0219                        14251 non-null  float32\n",
      " 267  zz0220                        14251 non-null  float32\n",
      " 268  zz0221                        14251 non-null  float32\n",
      " 269  zz0222                        14251 non-null  float32\n",
      " 270  zz0223                        14251 non-null  float32\n",
      " 271  zz0224                        14251 non-null  float32\n",
      " 272  zz0225                        14251 non-null  float32\n",
      " 273  zz0226                        14251 non-null  float32\n",
      " 274  zz0227                        14251 non-null  float32\n",
      " 275  zz0228                        14251 non-null  float32\n",
      " 276  zz0229                        14251 non-null  float32\n",
      " 277  zz0230                        14251 non-null  float32\n",
      " 278  zz0231                        14251 non-null  float32\n",
      " 279  zz0232                        14251 non-null  float32\n",
      " 280  zz0233                        14251 non-null  float32\n",
      " 281  zz0234                        14251 non-null  float32\n",
      " 282  zz0235                        14251 non-null  float32\n",
      " 283  zz0236                        14251 non-null  float32\n",
      " 284  zz0237                        14251 non-null  float32\n",
      " 285  zz0238                        14251 non-null  float32\n",
      " 286  zz0239                        14251 non-null  float32\n",
      " 287  zz0240                        14251 non-null  float32\n",
      " 288  zz0241                        14251 non-null  float32\n",
      " 289  zz0242                        14251 non-null  float32\n",
      " 290  zz0243                        14251 non-null  float32\n",
      " 291  zz0244                        14251 non-null  float32\n",
      " 292  zz0245                        14251 non-null  float32\n",
      " 293  zz0246                        14251 non-null  float32\n",
      " 294  zz0247                        14251 non-null  float32\n",
      " 295  zz0248                        14251 non-null  float32\n",
      " 296  zz0249                        14251 non-null  float32\n",
      " 297  zz0250                        14251 non-null  float32\n",
      " 298  zz0251                        14251 non-null  float32\n",
      " 299  zz0252                        14251 non-null  float32\n",
      " 300  zz0253                        14251 non-null  float32\n",
      " 301  zz0254                        14251 non-null  float32\n",
      " 302  zz0255                        14251 non-null  float32\n",
      " 303  zz0256                        14251 non-null  float32\n",
      " 304  zz0257                        14251 non-null  float32\n",
      " 305  zz0258                        14251 non-null  float32\n",
      " 306  zz0259                        14251 non-null  float32\n",
      " 307  zz0260                        14251 non-null  float32\n",
      " 308  zz0261                        14251 non-null  float32\n",
      " 309  zz0262                        14251 non-null  float32\n",
      " 310  zz0263                        14251 non-null  float32\n",
      " 311  zz0264                        14251 non-null  float32\n",
      " 312  zz0265                        14251 non-null  float32\n",
      " 313  zz0266                        14251 non-null  float32\n",
      " 314  zz0267                        14251 non-null  float32\n",
      " 315  zz0268                        14251 non-null  float32\n",
      " 316  zz0269                        14251 non-null  float32\n",
      " 317  zz0270                        14251 non-null  float32\n",
      " 318  zz0271                        14251 non-null  float32\n",
      " 319  zz0272                        14251 non-null  float32\n",
      " 320  zz0273                        14251 non-null  float32\n",
      " 321  zz0274                        14251 non-null  float32\n",
      " 322  zz0275                        14251 non-null  float32\n",
      " 323  zz0276                        14251 non-null  float32\n",
      " 324  zz0277                        14251 non-null  float32\n",
      " 325  zz0278                        14251 non-null  float32\n",
      " 326  zz0279                        14251 non-null  float32\n",
      " 327  zz0280                        14251 non-null  float32\n",
      " 328  zz0281                        14251 non-null  float32\n",
      " 329  zz0282                        14251 non-null  float32\n",
      " 330  zz0283                        14251 non-null  float32\n",
      " 331  zz0284                        14251 non-null  float32\n",
      " 332  zz0285                        14251 non-null  float32\n",
      " 333  zz0286                        14251 non-null  float32\n",
      " 334  zz0287                        14251 non-null  float32\n",
      " 335  zz0288                        14251 non-null  float32\n",
      " 336  zz0289                        14251 non-null  float32\n",
      " 337  zz0290                        14251 non-null  float32\n",
      " 338  zz0291                        14251 non-null  float32\n",
      " 339  zz0292                        14251 non-null  float32\n",
      " 340  zz0293                        14251 non-null  float32\n",
      " 341  zz0294                        14251 non-null  float32\n",
      " 342  zz0295                        14251 non-null  float32\n",
      " 343  zz0296                        14251 non-null  float32\n",
      " 344  zz0297                        14251 non-null  float32\n",
      " 345  zz0298                        14251 non-null  float32\n",
      " 346  zz0299                        14251 non-null  float32\n",
      " 347  zz0300                        14251 non-null  float32\n",
      " 348  zz0301                        14251 non-null  float32\n",
      " 349  zz0302                        14251 non-null  float32\n",
      " 350  zz0303                        14251 non-null  float32\n",
      " 351  zz0304                        14251 non-null  float32\n",
      " 352  zz0305                        14251 non-null  float32\n",
      " 353  zz0306                        14251 non-null  float32\n",
      " 354  zz0307                        14251 non-null  float32\n",
      " 355  zz0308                        14251 non-null  float32\n",
      " 356  zz0309                        14251 non-null  float32\n",
      " 357  zz0310                        14251 non-null  float32\n",
      " 358  zz0311                        14251 non-null  float32\n",
      " 359  zz0312                        14251 non-null  float32\n",
      " 360  zz0313                        14251 non-null  float32\n",
      " 361  zz0314                        14251 non-null  float32\n",
      " 362  zz0315                        14251 non-null  float32\n",
      " 363  zz0316                        14251 non-null  float32\n",
      " 364  zz0317                        14251 non-null  float32\n",
      " 365  zz0318                        14251 non-null  float32\n",
      " 366  zz0319                        14251 non-null  float32\n",
      " 367  zz0320                        14251 non-null  float32\n",
      " 368  zz0321                        14251 non-null  float32\n",
      " 369  zz0322                        14251 non-null  float32\n",
      " 370  zz0323                        14251 non-null  float32\n",
      " 371  zz0324                        14251 non-null  float32\n",
      " 372  zz0325                        14251 non-null  float32\n",
      " 373  zz0326                        14251 non-null  float32\n",
      " 374  zz0327                        14251 non-null  float32\n",
      " 375  zz0328                        14251 non-null  float32\n",
      " 376  zz0329                        14251 non-null  float32\n",
      " 377  zz0330                        14251 non-null  float32\n",
      " 378  zz0331                        14251 non-null  float32\n",
      " 379  zz0332                        14251 non-null  float32\n",
      " 380  zz0333                        14251 non-null  float32\n",
      " 381  zz0334                        14251 non-null  float32\n",
      " 382  zz0335                        14251 non-null  float32\n",
      " 383  zz0336                        14251 non-null  float32\n",
      " 384  zz0337                        14251 non-null  float32\n",
      " 385  zz0338                        14251 non-null  float32\n",
      " 386  zz0339                        14251 non-null  float32\n",
      " 387  zz0340                        14251 non-null  float32\n",
      " 388  zz0341                        14251 non-null  float32\n",
      " 389  zz0342                        14251 non-null  float32\n",
      " 390  zz0343                        14251 non-null  float32\n",
      " 391  zz0344                        14251 non-null  float32\n",
      " 392  zz0345                        14251 non-null  float32\n",
      " 393  zz0346                        14251 non-null  float32\n",
      " 394  zz0347                        14251 non-null  float32\n",
      " 395  zz0348                        14251 non-null  float32\n",
      " 396  zz0349                        14251 non-null  float32\n",
      " 397  zz0350                        14251 non-null  float32\n",
      " 398  zz0351                        14251 non-null  float32\n",
      " 399  zz0352                        14251 non-null  float32\n",
      " 400  zz0353                        14251 non-null  float32\n",
      " 401  zz0354                        14251 non-null  float32\n",
      " 402  zz0355                        14251 non-null  float32\n",
      " 403  zz0356                        14251 non-null  float32\n",
      " 404  zz0357                        14251 non-null  float32\n",
      " 405  zz0358                        14251 non-null  float32\n",
      " 406  zz0359                        14251 non-null  float32\n",
      " 407  zz0360                        14251 non-null  float32\n",
      " 408  zz0361                        14251 non-null  float32\n",
      " 409  zz0362                        14251 non-null  float32\n",
      " 410  zz0363                        14251 non-null  float32\n",
      " 411  zz0364                        14251 non-null  float32\n",
      " 412  zz0365                        14251 non-null  float32\n",
      " 413  zz0366                        14251 non-null  float32\n",
      " 414  zz0367                        14251 non-null  float32\n",
      " 415  zz0368                        14251 non-null  float32\n",
      " 416  zz0369                        14251 non-null  float32\n",
      " 417  zz0370                        14251 non-null  float32\n",
      " 418  zz0371                        14251 non-null  float32\n",
      " 419  zz0372                        14251 non-null  float32\n",
      " 420  zz0373                        14251 non-null  float32\n",
      " 421  zz0374                        14251 non-null  float32\n",
      " 422  zz0375                        14251 non-null  float32\n",
      " 423  zz0376                        14251 non-null  float32\n",
      " 424  zz0377                        14251 non-null  float32\n",
      " 425  zz0378                        14251 non-null  float32\n",
      " 426  zz0379                        14251 non-null  float32\n",
      " 427  zz0380                        14251 non-null  float32\n",
      " 428  zz0381                        14251 non-null  float32\n",
      " 429  zz0382                        14251 non-null  float32\n",
      " 430  zz0383                        14251 non-null  float32\n",
      "dtypes: float32(429), int16(1), object(1)\n",
      "memory usage: 23.5+ MB\n"
     ]
    }
   ],
   "source": [
    "cols = [\"text\"] + cols + em_cols\n",
    "df[cols].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b13fb00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 497 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df[cols].to_parquet(\"output/val.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c893b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
