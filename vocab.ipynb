{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99880378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71f820bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles=[.01, .05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99]\n",
    "pd.set_option(\"use_inf_as_na\", True)\n",
    "pd.set_option(\"max_info_columns\", 9999)\n",
    "pd.set_option(\"display.max_columns\", 9999)\n",
    "pd.set_option(\"display.max_rows\", 9999)\n",
    "pd.set_option('max_colwidth', 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bad991f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\", exclude=[\"textcat\"])\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e47fcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 658 entries, 0 to 657\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   term    658 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 5.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"input/hatevocabraw.tsv\", header=0, names=[\"term\"], engine=\"c\", low_memory=False)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c38b0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>african</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>african american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alabama hot pocket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alaskan pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 term\n",
       "0             african\n",
       "1    african american\n",
       "2  alabama hot pocket\n",
       "3    alaskan pipeline\n",
       "4            american"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "459392c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(terms)=550\n",
      "['african', 'african american', 'alabama hot pocket', 'alaskan pipeline', 'american', 'anal', 'analplug', 'analsex', 'anilingus', 'anus', 'apeshit', 'arse', 'arsehole', 'asian', 'ass', 'assassin', 'asshole', 'assmunch', 'atheist', 'auto erotic', 'autoerotic', 'babeland', 'baby batter', 'baby juice', 'ball', 'ball gag', 'ball gravy', 'ball kicking', 'ball lick', 'ball sack', 'ball suck', 'bangbro', 'bangbus', 'bareback', 'barely legal', 'barenaked', 'bastard', 'bastardo', 'bastinado', 'bbw', 'bdsm', 'beaner', 'beastiality', 'beaver cleaver', 'beaver lip', 'bestiality', 'bewb', 'big black', 'big breast', 'big knocker', 'big tit', 'bimbo', 'birdlock', 'bisexual', 'bitch', 'black', 'black cock', 'blind', 'blonde action', 'bloody', 'bloodyhell', 'blow', 'blow job', 'blow your load', 'blowjob', 'blue waffle', 'blumpkin', 'bollock', 'bondage', 'boner', 'boob', 'booby', 'booty call', 'boy', 'brown shower', 'brunette action', 'buddhist', 'bugger', 'bukkake', 'bulldyke', 'bullet vibe', 'bullshit', 'bung hole', 'bunghole', 'busty', 'butt', 'buttcheek', 'butthole', 'camel toe', 'camgirl', 'camslut', 'camwhore', 'canadian', 'carpet muncher', 'carpetmuncher', 'catholic', 'cawk', 'chinese', 'chink', 'choad', 'chocolate rosebud', 'christian', 'cialis', 'circlejerk', 'cleveland steamer', 'clit', 'clitoris', 'clover clamp', 'clusterfuck', 'cock', 'cocksucker', 'condom', 'coon', 'cooter', 'coprolagnia', 'coprophilia', 'cornhole', 'crap', 'creampie', 'cum', 'cumme', 'cumshot', 'cunnilingus', 'cunt', 'damm', 'dammit', 'damn', 'darkie', 'date rape', 'daterape', 'deaf', 'deep throat', 'deepthroat', 'dendrophilia', 'dick', 'dickhead', 'dildo', 'dingleberrie', 'dingleberry', 'dirty pillow', 'dirty sanchez', 'dog style', 'doggie style', 'doggiestyle', 'doggy style', 'doggystyle', 'dolcett', 'domination', 'dominatrix', 'domme', 'dong', 'donkey punch', 'double dong', 'double penetration', 'douche', 'dp action', 'dry hump', 'dvda', 'dyke', 'eat my ass', 'ecchi', 'ejaculation', 'elderly', 'erotic', 'erotism', 'escort', 'eunuch', 'european', 'f0ck', 'fag', 'faggot', 'fanny', 'fart', 'fck', 'fcker', 'fckr', 'fcku', 'fcuk', 'fecal', 'felch', 'fellatio', 'feltch', 'female', 'female squirting', 'femdom', 'feminist', 'figging', 'finger', 'fingerbang', 'fisting', 'foot fetish', 'footjob', 'foreskin', 'frotte', 'fuck', 'fuck button', 'fucker', 'fuckface', 'fuckin', 'fucking', 'fuckr', 'fucktard', 'fuct', 'fudge packer', 'fudgepacker', 'fuk', 'futanari', 'g - spot', 'gang bang', 'gangbang', 'gay', 'gay sex', 'genital', 'genitalia', 'giant cock', 'girl', 'girl go wild', 'girl on', 'girl on top', 'glory hole', 'gloryhole', 'goatcx', 'goatse', 'gobshite', 'god damn', 'godammet', 'godammit', 'goddammet', 'goddammit', 'goddamn', 'gokkun', 'golden shower', 'goo girl', 'goodpoop', 'gook', 'goregasm', 'grope', 'group sex', 'guro', 'gypo', 'hand job', 'handjob', 'hard core', 'hardcore', 'hell', 'hentai', 'heterosexual', 'hispanic', 'hitler', 'homo', 'homoerotic', 'homosexual', 'honkey', 'hooker', 'hore', 'horny', 'hot carl', 'hot chick', 'how to kill', 'how to murder', 'huge fat', 'hump', 'incest', 'indian', 'intercourse', 'islam', 'jack off', 'jail bait', 'jailbait', 'japanese', 'jelly donut', 'jerk off', 'jesussuck', 'jewish', 'jigaboo', 'jiggaboo', 'jiggerboo', 'jiz', 'jizz', 'jizzum', 'juggs', 'kaffir', 'kike', 'kill', 'killer', 'killin', 'kinbaku', 'kinkster', 'kinky', 'knob', 'kunt', 'labia', 'latina', 'latino', 'latinx', 'leather restraint', 'leather straight jacket', 'lemon party', 'lesbian', 'lesbo', 'lgbt', 'lgbtq', 'livesex', 'lolita', 'lovemake', 'make i come', 'male', 'male squirting', 'man', 'masturbate', 'masturbation', 'menage a trois', 'mexican', 'middle aged', 'middle eastern', 'milf', 'millennial', 'missionary position', 'molest', 'mong', 'moron', 'motherfuck', 'motherfucker', 'mound of venus', 'mr hand', 'mthrfckr', 'muff', 'muff diver', 'muffdive', 'murder', 'murderer', 'muslim', 'nambla', 'nawashi', 'nazi', 'negro', 'neonazi', 'nig nog', 'nigga', 'niggah', 'nigger', 'nimphomania', 'nipple', 'nonbinary', 'nonce', 'nsfw', 'nsfw image', 'nude', 'nudity', 'nutsack', 'nutten', 'nympho', 'nymphomania', 'octopussy', 'old', 'omorashi', 'orgasm', 'orgy', 'paedo', 'paedophile', 'paki', 'pantie', 'panty', 'paralyze', 'pecker', 'pedo', 'pedobear', 'pedofile', 'pedophile', 'peg', 'pen1s', 'penis', 'phone sex', 'phuk', 'piece of shit', 'pig', 'pikey', 'pimp', 'piss', 'piss pig', 'pisspig', 'playboy', 'pleasure chest', 'pole smoker', 'ponyplay', 'poof', 'poon', 'poontang', 'poop', 'poop chute', 'poopchute', 'porn', 'porno', 'pornography', 'prick', 'prince albert piercing', 'pron', 'prostitute', 'protestant', 'pthc', 'pubis', 'punani', 'punany', 'pussy', 'queaf', 'queef', 'queer', 'quim', 'rage boner', 'raghead', 'rape', 'rapist', 'rectal', 'rectum', 'reverse cowgirl', 'rim', 'rimjob', 'rosy palm', 'rusty trombone', 's&m', 'sadism', 'santorum', 'scat', 'schlong', 'scissor', 'screw', 'scrotum', 'semen', 'sex', 'sexcam', 'sexo', 'sexual', 'sexuality', 'sexually', 'sexy', 'shag', 'shave beaver', 'shave pussy', 'shemale', 'shibari', 'shit', 'shitblimp', 'shite', 'shitty', 'shiz', 'shota', 'shrimping', 'sikh', 'skeet', 'slag', 'slanteye', 'slut', 'smut', 'snatch', 'snowballing', 'sockpuppet', 'sodomize', 'sodomy', 'spastic', 'spaz', 'sperm', 'spic', 'spick', 'splooge', 'splooge moose', 'spoo', 'spooge', 'spread leg', 'spunk', 'straight', 'strap on', 'strapon', 'strappado', 'strip club', 'stripper', 'style doggy', 'suck', 'suicide girl', 'sultry woman', 'swastika', 'swinger', 'taint', 'taint love', 'taoist', 'tart', 'taste my', 'tea bagging', 'teenage', 'terrorist', 'threesome', 'throate', 'thumbzilla', 'tie up', 'tight white', 'tit', 'titty', 'tittyfuck', 'tongue in a', 'topless', 'tosser', 'towelhead', 'tran', 'tranny', 'transgender', 'tribadism', 'tub girl', 'tubgirl', 'turd', 'tushy', 'twat', 'twink', 'twinkie', 'undress', 'upskirt', 'urethra play', 'urophilia', 'vag', 'vagina', 'vaginal', 'venus mound', 'viagra', 'vibrator', 'violet wand', 'vorarephilia', 'voyeur', 'voyeurweb', 'voyuer', 'vulva', 'wank', 'wanker', 'weed', 'wet dream', 'wetback', 'white', 'white power', 'whor', 'whore', 'wog', 'woman', 'worldsex', 'wrap man', 'wrinkled starfish', 'wtf', 'xx', 'xxx', 'yaoi', 'yellow shower', 'yiffy', 'young', 'zoophilia']\n"
     ]
    }
   ],
   "source": [
    "terms = list(df[\"term\"])\n",
    "tmp = []\n",
    "for t in terms:\n",
    "    t = str(t).strip()\n",
    "    tmp.append(t)\n",
    "terms = tmp\n",
    "tmp = []\n",
    "for doc in nlp.pipe(terms):\n",
    "    if len(doc) > 3:\n",
    "        continue\n",
    "    t = \" \".join([token.lemma_ for token in doc])\n",
    "    t = t.lower()\n",
    "    tmp.append(t)\n",
    "terms = list(set(tmp))\n",
    "terms.sort()\n",
    "print(f\"len(terms)={len(terms)}\\n{terms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5084ca57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"input/pre_ruddit.parquet\")\n",
    "texts = list(df[\"text3\"])\n",
    "df = pd.read_parquet(\"input/pre_val.parquet\")\n",
    "texts += list(df[\"text3\"])\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0102663e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf.shape=(550,)\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vec = TfidfVectorizer(vocabulary=terms, ngram_range=(1, 3), analyzer=\"word\")\n",
    "vec = vec.fit(texts)\n",
    "print(f\"idf.shape={vec.idf_.shape}\")\n",
    "idf = vec.idf_.tolist()\n",
    "with open(\"output/vocab.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"term\": terms,\n",
    "        \"idf\": idf,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1414caaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.98 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('output/vocab.tsv', 'w', newline='') as f:\n",
    "    w = csv.writer(f, delimiter='\\t')\n",
    "    w.writerow([\"term\", \"idf\"])\n",
    "    for i in range(len(terms)):\n",
    "        w.writerow([terms[i], idf[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f00d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
